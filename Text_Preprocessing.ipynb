{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWyY2GFpxrLP"
   },
   "source": [
    "# Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4fRngJYx35I"
   },
   "source": [
    "**Step 1: Connect to Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyxY_iDNL7eR",
    "outputId": "f6d77b43-79e4-4642-fbf8-8ff7f0599307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5jmPCwpyGll"
   },
   "source": [
    "**Step 2: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHzND_vKY2K6",
    "outputId": "acec09df-09fc-4dc6-e0d8-9b21170debf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gc\n",
    "from smart_open import open\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, LSTM, Dense, Attention, TimeDistributed, Sequential, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Dot, Activation, Multiply, Permute, RepeatVector, Lambda, Softmax\n",
    "from keras.layers import Layer, Reshape\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUmcBAnFEa3e"
   },
   "source": [
    "**Step 3: Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZO_RYd4YPlek"
   },
   "outputs": [],
   "source": [
    "dataset1 = np.array(pd.read_csv('/content/drive/My Drive/dev_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
    "dataset2 = np.array(pd.read_csv('/content/drive/My Drive/full_test_split.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
    "dataset3 = np.array(pd.read_csv('/content/drive/My Drive/train_split_Depression_AVEC2017.csv',delimiter=',',encoding='utf-8'))[:, 0:2]\n",
    "\n",
    "dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n",
    "countPos = 0\n",
    "\n",
    "def checkPosNeg(dataset, index):\n",
    "    for i in range(0, len(dataset)):\n",
    "        if(dataset[i][0] == index):\n",
    "            return dataset[i][1]\n",
    "    return 0\n",
    "\n",
    "Data = []\n",
    "Y = []\n",
    "\n",
    "countPos = 0\n",
    "index = -1\n",
    "Data_test = []\n",
    "Y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7iYyAjaksUY"
   },
   "outputs": [],
   "source": [
    "text_train_ids = [int(item[0]) for item in dataset3]\n",
    "text_dev_ids = [int(item[0]) for item in dataset1]\n",
    "text_test_ids = [int(item[0]) for item in dataset2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XDYf6pbz1Nb"
   },
   "source": [
    "**Step 4: Obtaining Corresponding Transcript**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtHp-ThyPp_M"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0, len(dataset3)):\n",
    "    val = checkPosNeg(dataset, dataset3[i][0])\n",
    "    Y.append(val)\n",
    "    try:\n",
    "        fileName = \"/content/drive/My Drive/\" + str(int(dataset3[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "        Data.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))[:, 2:4])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr1sPuA8Q0YU"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(dataset1)):\n",
    "    val = checkPosNeg(dataset, dataset1[i][0])\n",
    "    Y.append(val)\n",
    "    try:\n",
    "        fileName = \"/content/drive/My Drive/\" + str(int(dataset1[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "        Data.append(np.array(pd.read_csv(fileName, delimiter='\\t', encoding='utf-8', engine='python'))[:, 2:4])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOk7pY1U4w9r",
    "outputId": "94b07730-5676-4f9c-8147-1fbf9a5d1f44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-9c111b65c987>:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Data2 = np.array(Data2)\n",
      "<ipython-input-10-9c111b65c987>:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Data2_test = np.array(Data2_test)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, len(dataset2)):\n",
    "    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n",
    "    try:\n",
    "        fileName = \"/content/drive/My Drive/\" + str(int(dataset2[i][0])) + \"_TRANSCRIPT.csv\"\n",
    "        Data_test.append(np.array(pd.read_csv(fileName,delimiter='\\t',encoding='utf-8', engine='python'))[:, 2:4])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "Y = np.array(Y)\n",
    "Data2 = []\n",
    "\n",
    "Data2_test = []\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "for i in range(0, len(Data)):\n",
    "    script = []\n",
    "    for k in range(1, len(Data[i])):\n",
    "        if(Data[i][k][0] == \"Participant\"):\n",
    "            script.append(Data[i][k][1])\n",
    "    Data2.append(script)\n",
    "\n",
    "for i in range(0, len(Data_test)):\n",
    "    script = []\n",
    "    for k in range(1, len(Data_test[i])):\n",
    "        if(Data_test[i][k][0] == \"Participant\"):\n",
    "            script.append(Data_test[i][k][1])\n",
    "    Data2_test.append(script)\n",
    "\n",
    "Data = []\n",
    "Data_test = []\n",
    "gc.collect()\n",
    "Data2 = np.array(Data2)\n",
    "Data2_test = np.array(Data2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLyH5fELzR-9"
   },
   "source": [
    "**Step 5: Text Preprocessing using word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tnfza1e85qvI",
    "outputId": "f96d0842-db4a-4d6c-ade7-576a5d141bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 250, 20, 300)\n",
      "(142,)\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def Thresholding(Y_pred, threshold):\n",
    "  Y_pred2 = []\n",
    "  for i in range(len(Y_pred)):\n",
    "    if(Y_pred[i] < threshold):\n",
    "      Y_pred2.append(0)\n",
    "    else:\n",
    "      Y_pred2.append(1)\n",
    "\n",
    "  return np.array(Y_pred2)\n",
    "\n",
    "def remove_StopWords(sentence):\n",
    "    filtered_sentence = []\n",
    "    for w in sentence:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return filtered_sentence\n",
    "\n",
    "def checkAcc(Y_pred, Y_test):\n",
    "    correct = 0\n",
    "    for i in range(len(Y_pred)):\n",
    "        if(Y_pred[i] == Y_test[i]):\n",
    "            correct+=1\n",
    "\n",
    "    return float(correct)/len(Y_pred)\n",
    "\n",
    "def upsample(X_train,Y_train):\n",
    "    # Separate majority and minority classes\n",
    "    X_train_0 = X_train[Y_train==0]\n",
    "    X_train_1 = X_train[Y_train==1]\n",
    "\n",
    "    # Compute the difference in size\n",
    "    size = X_train_0.shape[0] - X_train_1.shape[0]\n",
    "\n",
    "    # If the minority class is smaller, upsample it\n",
    "    if size > 0:\n",
    "      # Randomly select 'size' samples from the minority class with replacement\n",
    "      indices = np.random.choice(X_train_1.shape[0], size=size, replace=True)\n",
    "      X_upsampled = X_train_1[indices]\n",
    "      Y_upsampled = np.ones(size)\n",
    "\n",
    "      # Concatenate the upsampled minority class and the original majority class\n",
    "      X_train = np.concatenate((X_train_0, X_train_1, X_upsampled))\n",
    "      Y_train = np.concatenate((np.zeros(X_train_0.shape[0]), np.ones(X_train_1.shape[0]), Y_upsampled))\n",
    "\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "\n",
    "max_num_words = 20\n",
    "max_num_sentence = 250\n",
    "\n",
    "#train_data\n",
    "finalMatrix = np.zeros((Data2.shape[0], max_num_sentence, max_num_words, 300))\n",
    "max_length_sent = 0\n",
    "sent = \"\"\n",
    "for k in range(Data2.shape[0]):\n",
    "    if(max_length_sent < len(Data2[k])):\n",
    "      max_length_sent = len(Data2[k])\n",
    "      sent = Data2[k]\n",
    "    for i in range(min(max_num_sentence, len(Data2[k]))):\n",
    "    \ttry:\n",
    "    \t  sentence = Data2[k][i].split(\" \")\n",
    "    \texcept:\n",
    "    \t  continue\n",
    "    \tsentence = remove_StopWords(sentence)\n",
    "    \tfor j in range(min(max_num_words, len(sentence))):\n",
    "    \t\ttry:\n",
    "    \t\t  word = sentence[j]\n",
    "    \t\t  # print(\"Before\", word)\n",
    "    \t\t  if(word[0] == '<'):\n",
    "    \t\t    if(word.find('>')!=-1):\n",
    "    \t\t      word = word[1:-1]\n",
    "    \t\t    else:\n",
    "    \t\t      word = word[1:]\n",
    "    \t\t  else:\n",
    "    \t\t    if(word.find('>')!=-1):\n",
    "    \t\t      word = word[0:-1]\n",
    "    \t\t  finalMatrix[k][i][j] = np.array(model[word])\n",
    "    \t\texcept Exception as e:\n",
    "    \t\t\tcontinue\n",
    "\n",
    "#Test_data\n",
    "max_length_sent = 0\n",
    "finalMatrix_test = np.zeros((Data2_test.shape[0], max_num_sentence, max_num_words, 300))\n",
    "# print(finalMatrix_test.shape)\n",
    "for k in range(Data2_test.shape[0]):\n",
    "    if(max_length_sent < len(Data2_test[k])):\n",
    "      max_length_sent = len(Data2_test[k])\n",
    "      sent = Data2_test[k]\n",
    "    for i in range(min(max_num_sentence, len(Data2_test[k]))):\n",
    "    \ttry:\n",
    "    \t  sentence = Data2_test[k][i].split(\" \")\n",
    "    \texcept:\n",
    "    \t  continue\n",
    "    \tsentence = remove_StopWords(sentence)\n",
    "    \tfor j in range(min(max_num_words, len(sentence))):\n",
    "    \t\ttry:\n",
    "    \t\t  word = sentence[j]\n",
    "    \t\t  if(word[0] == '<'):\n",
    "    \t\t    if(word.find('>')!=-1):\n",
    "    \t\t      word = word[1:-1]\n",
    "    \t\t    else:\n",
    "    \t\t      word = word[1:]\n",
    "    \t\t  else:\n",
    "    \t\t    if(word.find('>')!=-1):\n",
    "    \t\t      word = word[0:-1]\n",
    "    \t\t  finalMatrix_test[k][i][j] = np.array(model[word])\n",
    "    \t\texcept Exception as e:\n",
    "    \t\t\tcontinue\n",
    "\n",
    "Data2 = []\n",
    "Data2_test = []\n",
    "model = []\n",
    "stop_words = []\n",
    "gc.collect()\n",
    "print(finalMatrix.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQBpAGIHmhgz",
    "outputId": "068f76af-18d5-4fea-a07a-bd7456093f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 250, 20, 300)\n",
      "(142,)\n"
     ]
    }
   ],
   "source": [
    "print(finalMatrix.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVWvR3VHsc7t"
   },
   "outputs": [],
   "source": [
    "finalMatrix_ids = [int(item[0]) for item in finalMatrix]\n",
    "finalMatrix_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54UGp2E4n9jt",
    "outputId": "aeb19048-58f5-48e8-9495-ea99514145cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgApDxY10RPK"
   },
   "source": [
    "**Step 6: Class Balancing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bm-X4N610V-L"
   },
   "outputs": [],
   "source": [
    "finalMatrix, Y = upsample(finalMatrix,Y)\n",
    "finalMatrix_test, Y_test = upsample(finalMatrix_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiwRHyyWy62r"
   },
   "source": [
    "**Step 7: Save Preprocessed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vcv90oEQmUcb"
   },
   "outputs": [],
   "source": [
    "np.save(file=\"/content/drive/My Drive/finalMatrix_1.npy\", arr=finalMatrix)\n",
    "np.save(file=\"/content/drive/My Drive/text_Y_1.npy\", arr=Y)\n",
    "np.save(file=\"/content/drive/My Drive/finalMatrix_test.npy\", arr=finalMatrix_test)\n",
    "np.save(file=\"/content/drive/My Drive/text_Y_test.npy\", arr=Y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
